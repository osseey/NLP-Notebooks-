{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJAJ6CYYmy0k"
      },
      "outputs": [],
      "source": [
        "# Sentiment analysis on IMDB movie reviews with Na√Øve Bayes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9zx_Y1km284"
      },
      "outputs": [],
      "source": [
        "import os.path as op\n",
        "import re \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9XEBJ9Ym3At"
      },
      "outputs": [],
      "source": [
        "## Loading data\n",
        "#We retrieve the textual data in the variable *texts*\n",
        "#The labels are retrieved in the variable $y$\n",
        "#- it contains *len(texts)* of them: \n",
        "#$0$ indicates that the corresponding review is negative \n",
        "#while $1$ indicates that it is positive.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCJaY0Iqm3F9"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "    # We get the files from the path: ./aclImdb/train/neg for negative reviews, and ./aclImdb/train/pos for positive reviews\n",
        "#train_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'train', 'neg', '*.txt')))\n",
        "#train_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'train', 'pos', '*.txt')))\n",
        "train_filenames_neg = sorted(glob(op.join('.', 'neg', '*.txt')))\n",
        "train_filenames_pos = sorted(glob(op.join('.', 'pos', '*.txt')))\n",
        "\n",
        "#test_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'test', 'neg', '*.txt')))\n",
        "#test_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'test', 'pos', '*.txt')))\n",
        "\n",
        "#test_filenames_neg = sorted(glob(op.join('.', 'neg', '*.txt')))\n",
        "#test_filenames_pos = sorted(glob(op.join('.', 'pos', '*.txt')))   \n",
        "\n",
        "\n",
        "    # Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
        "train_texts_neg = [open(f, encoding=\"utf8\").read() for f in train_filenames_neg]\n",
        "train_texts_pos = [open(f, encoding=\"utf8\").read() for f in train_filenames_pos]\n",
        "train_texts = train_texts_neg + train_texts_pos\n",
        "#test_texts_neg = [open(f, encoding=\"utf8\").read() for f in test_filenames_neg]\n",
        "#test_texts_pos = [open(f, encoding=\"utf8\").read() for f in test_filenames_pos]\n",
        "#test_texts = test_texts_neg + test_texts_pos\n",
        "\n",
        "    # The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
        "    # We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to\n",
        "\n",
        "train_labels = np.ones(len(train_texts), dtype=int)\n",
        "train_labels[:len(train_texts_neg)] = 0\n",
        "#test_labels = np.ones(len(test_texts), dtype=int)\n",
        "#test_labels[:len(test_texts_neg)] = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARUf42EL-59D"
      },
      "outputs": [],
      "source": [
        "#open(\"/neg/0_3.txt\", encoding=\"utf8\").read()\n",
        "#In this lab, the impact of our choice of representations\n",
        "#upon our results will also depend on the quantity of data we use:** \n",
        "#try to see how changing the parameter ```k``` affects our results !\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrFCJJU0_fFV",
        "outputId": "72eb41f8-9dab-4f3d-b442-d3b11b6a8778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 2500\n"
          ]
        }
      ],
      "source": [
        "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "\n",
        "k = 10\n",
        "train_texts_reduced = train_texts[0::k]\n",
        "train_labels_reduced = train_labels[0::k]\n",
        "print('Number of documents:', len(train_texts_reduced))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj6EXhQ7_fNR"
      },
      "outputs": [],
      "source": [
        "#We can use a function from sklearn, ```train_test_split```, to separate data into training and validation sets:\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts_splt, val_texts, train_labels_splt, val_labels = train_test_split(train_texts_reduced, train_labels_reduced, test_size=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3-qYauXAaJR"
      },
      "outputs": [],
      "source": [
        "## Adapted representation of documents\\n\",\n",
        "\n",
        "#Our statistical model, like most models applied to textual data, uses counts of word occurrences \n",
        "#in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, \n",
        "#containing the counts of each word (regardless of their order of occurrence) in the document.\n",
        "\n",
        "#If we consider the set of all the words appearing in our $T$ training documents, \n",
        "#which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to \n",
        "#each $w$ word an integer, which will be its position in $V$.\n",
        "#Thus, for a document extracted from a set of documents containing $|V|$ different words, \n",
        "#a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ \n",
        "#will be its number of occurrences in the document\n",
        "\n",
        "#We can use the **CountVectorizer** class from scikit-learn to obtain these representations:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCf1hWFVAaOD",
        "outputId": "f9e6cd37-6dae-44e8-eb86-b7ba7585c71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 2, 1, 0]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "corpus = ['I walked down down the boulevard',\n",
        "              'I walked down the avenue',\n",
        "              'I ran down the boulevard',\n",
        "              'I walk down the city',\n",
        "             'I walk down the the avenue']\n",
        "vectorizer = CountVectorizer()\n",
        "Bow = vectorizer.fit_transform(corpus)\n",
        "# Vocabulary of the corpus\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "# Vecteur Bag of words\n",
        "Bow.toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSxJ3mP3BOxY"
      },
      "outputs": [],
      "source": [
        "#We display the list containing the words ordered according to their index (Note that words\n",
        "#of 2 characters or less are not counted)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD5KjSyQBO2G"
      },
      "outputs": [],
      "source": [
        "## Preprocessing the text: get the BoW representations ##\\n\",\n",
        "\n",
        "#The first thing to do is to turn the review from a string into a list of words.\n",
        "#The simplest method is to divide the string according to spaces with the command:``text.split()``\n",
        "\n",
        "#But we must also be careful to remove special characters that may not have been cleaned up \n",
        "#(such as HTML tags if the data was obtained from web pages). Since we're going to count words, \n",
        "#we'll have to build a list of tokens appearing in our data. In our case, we'd like to reduce this\n",
        " #list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
        "#We will therefore use a function adapted to our needs - but this is a job that we generally don't\n",
        " #need to do ourselves, since there are many tools already adapted to most situations. \n",
        "#For text cleansing, there are many scripts, based on different tools (regular expressions, \n",
        "#for example) that allow you to prepare data. The division of the text into words and the management \n",
        "#of punctuation is handled in a step called *tokenization*; if needed, a python package like NLTK contains\n",
        "#many different *tokenizers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfN9AnxsJYFh",
        "outputId": "9348de66-f124-48a0-ca25-d1a22df9d1e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmjn8kbYBO8V",
        "outputId": "17d07297-ddaa-413d-a7ef-d5abaaacd6c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
            "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
          ]
        }
      ],
      "source": [
        "# We might want to clean the file with various strategies:\\n\",\n",
        "def clean_and_tokenize(text):\n",
        "\n",
        "           #Cleaning a document with:\\n\",\n",
        "    #        - Lowercase        \\n\",\n",
        "    #       - Removing numbers with regular expressions\\n\",\n",
        "    #        - Removing punctuation with regular expressions\\n\",\n",
        "    #        - Removing other artifacts\\n\",\n",
        "    #    And separate the document into words by simply splitting at spaces\\n\",\n",
        "    #    Params:\\n\",\n",
        "    #        text (string): a sentence or a document\\n\",\n",
        "    #    Returns:\\n\",\n",
        "    #        tokens (list of strings): the list of tokens (word units) forming the document\\n\",\n",
        "    #    \\\"\\\"\\\"     \n",
        "    \n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()[\\]]\")\n",
        "    text = REMOVE_PUNCT.sub(\"\", text)\n",
        "\n",
        "    # Remove small words (1 and 2 characters)\n",
        "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
        "\n",
        "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
        "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "    text = REPLACE_HTML.sub(\" \", text)\n",
        "\n",
        "    tokens = text.split()       \n",
        "    return tokens\n",
        "\n",
        "\n",
        "    # Or we might want to use an already-implemented tool. The NLTK package has a lot of very \n",
        "    #useful text processing tools, among them various tokenizers\n",
        "    # Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses.\n",
        "    #Check the documentation !\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "    \n",
        "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
        "    \n",
        "print(clean_and_tokenize(corpus_raw))\n",
        "print(word_tokenize(corpus_raw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMc8BrNXAaSs"
      },
      "outputs": [],
      "source": [
        "# The next function takes as input a list of documents (each in the form of a string) \n",
        "#and returns, as in the example using ``CountVectorizer``: a\n",
        "# vocabulary that associates, to each word encountered, an index,\n",
        "# A matrix, with rows representing documents and columns representing words indexed \n",
        "#by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the \n",
        "#word $j$ in the document $i$.\n",
        "# The vocabulary, which was in the form of a *list* in the previous example, \n",
        "#can be returned in the form of a *dictionary* whose keys are the words and values are the \n",
        "#indices. Since the vocabulary lists the words in the corpus without worrying about their number \n",
        "#of occurrences, it can be built up using a set (in python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L27aanQRAaWa"
      },
      "outputs": [],
      "source": [
        "def count_words(texts, voc = None):\n",
        "    #\"Vectorize text : return count of each word in the text snippets\\n\",\n",
        "    #Parameters\\n\",\n",
        "\n",
        "    # texts : list of str\\n\",\n",
        "    #      The texts\\n\",\n",
        "    \"    Returns\\n\",\n",
        "    \"    -------\\n\",\n",
        "    \"    vocabulary : dict\\n\",\n",
        "    \"        A dictionary that points to an index in counts for each word.\\n\",\n",
        "    \"    counts : ndarray, shape (n_samples, n_features)\\n\",\n",
        "    \"        The counts of each word in each text.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    n_samples = len(texts)\n",
        "    if voc == None:\n",
        "      words = set()\n",
        "      for text in texts:\n",
        "        words = words.union(set(clean_and_tokenize(text))) # list of all words,\n",
        "      n_features = len(words) # number of different words,\n",
        "      vocabulary = dict(zip(words, range(n_features))) # vocab[wd] = index ; indexation,\n",
        "    else:\n",
        "      vocabulary = voc\n",
        "      n_features = len(vocabulary)\n",
        "    counts = np.zeros((n_samples, n_features))\n",
        "    for k, text in enumerate(texts): # √©numeration: renvoie (k, texts[k]) ,\n",
        "      for w in clean_and_tokenize(text):\n",
        "        if w in vocabulary:\n",
        "          counts[k][vocabulary[w]] += 1.\n",
        "    return vocabulary, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC1NktrPZJp",
        "outputId": "f0221878-4882-49bb-8408-89205e04e9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 0, 'boulevard': 1, 'walk': 2, 'walked': 3, 'ran': 4, 'down': 5, 'avenue': 6, 'city': 7}\n",
            "[[1. 1. 0. 1. 0. 2. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [1. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [1. 0. 1. 0. 0. 1. 0. 1.]\n",
            " [2. 0. 1. 0. 0. 1. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "Voc, X = count_words(corpus)\n",
        "print(Voc)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANG3mqjCTVaw"
      },
      "outputs": [],
      "source": [
        "#Careful: check the memory that the representations are going to use \n",
        "#(given the way they are build). What ```CountVectorizer``` argument allows to avoid the issue ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAPNLIBAtQf1",
        "outputId": "c92720bc-1cdc-42af-c04e-2db218b405c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 105977)\n"
          ]
        }
      ],
      "source": [
        "voc, train_bow = count_words(train_texts)\n",
        "print(train_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xihZzgSTftr",
        "outputId": "1b7b8dca-9322-4140-938b-29e382936041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 105977)\n"
          ]
        }
      ],
      "source": [
        "_, val_bow = count_words(val_texts, voc)\n",
        "print(val_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipko8iqj9SsX",
        "outputId": "e4806b2e-1705-4d70-86e7-5d46c47d00a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 25495)\n"
          ]
        }
      ],
      "source": [
        "# Create and fit the vectorizer to the training data\n",
        "\n",
        "vectorizer=CountVectorizer(max_features=30000)\n",
        "Bow=vectorizer.fit_transform(train_texts_splt)\n",
        "train_bow=Bow.toarray()\n",
        "print(train_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0D00AS8vxBM",
        "outputId": "f5258425-925e-4d8f-89bc-7b1eb0b3a918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 25495)\n"
          ]
        }
      ],
      "source": [
        "# Transform the validation data\n",
        "\n",
        "val_bow = vectorizer.transform(val_texts).toarray()\n",
        "print(val_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6GI_TeSwGgq"
      },
      "outputs": [],
      "source": [
        "## Naive Bayesian \\n\",\n",
        "    ### Main idea\\n\",\n",
        " \n",
        "    #A movie review is in fact a list of words $s = (w_1, ..., w_N)$, \n",
        "    #and we try to find the associated class $c$ - which in our case may be $c = 0$ \n",
        "    #or $c = 1$. The objective is thus to find for each review $s$ the class $\\\\hat{c}$ \n",
        "    #maximizing the conditional probability **$P(c|s)$** : \\n\",\n",
        "\n",
        "    #$$\\\\hat{c} = \\\\underset{c}{\\\\mathrm{argmax}}\\\\, \n",
        "    #P(c|s) = \\\\underset{c}{\\\\mathrm{argmax}}\\\\,\n",
        "    #\\\\frac{P(s|c)P(c)}{P(s)}$$\\n\",\n",
        "\n",
        "    #**Hypothesis : P(s) is constant for each class** :\\n\",\n",
        "\n",
        "    #$$\\\\hat{c} = \\\\underset{c}{\\\\mathrm{argmax}}\\\\,\\\\frac{P(s|c)P(c)}{P(s)} = \\\\underset{c}{\\\\mathrm{argmax}}\\\\,P(s|c)P(c)$$\\n\",\n",
        "   \n",
        "    #**Naive hypothesis : the variables (words) of a review are independant between themselves** : \\n\",\n",
        "   \n",
        "    #$$P(s|c) = P(w_1, ..., w_N|c)=\\\\Pi_{i=1..N}P(w_i|c)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hht2lBLowGkS"
      },
      "outputs": [],
      "source": [
        "### General view\n",
        "   \n",
        "    #### Training: Estimating the probabilities\n",
        "    \n",
        "    #For each word $w$ in the vocabulary $V$, $P(w|c)$ is the number \n",
        "    #of occurrences of $w$ in all reviews of class $c$, divided by the total number of occurrences \n",
        "    #in $c$. \n",
        "    #If we note $T(w,c)$ this number of occurrences, we get:\n",
        "    \n",
        "    #$$P(w|c) = \\\\text{Frequency of }w\\\\text{ in }c = \\\\frac{T(w,c)}{\\\\sum_{w' \\\\in V} T(w',c)}$$\\n\",\n",
        "    \n",
        "    #### Test: Calculating scores\\n\",\n",
        "    \n",
        "    #To facilitate the calculations and to avoid *underflow* and approximation errors, we use the log-sum \n",
        "    #trick, and we pass the equation into log-probabilities :\n",
        "    \n",
        "    #$$ \\\\hat{c} = \\\\underset{c}{\\\\mathrm{argmax}} P(c|s) = \\\\underset{c}{\\\\mathrm{argmax}} \\\\left[ \\\\mathrm{log}(P(c)) + \\\\sum_{i=1..N}log(P(w_i|c)) \\\\right] $$\\n\",\n",
        "    \n",
        "    #### Laplace smoothing\\n\",\n",
        "    \n",
        "    #A word that does not appear in a document has a probability of zero: \n",
        "    #this will cause issues with the logarithm. So we keep a very small part of the \n",
        "    #probability mass that we redistribute with the *Laplace smoothing*: \\n\",\n",
        "    \n",
        "    #$$P(w|c) = \\\\frac{T(w,c) + 1}{\\\\sum_{w' \\\\in V} (T(w',c) + 1)}$$\\n\",\n",
        "   \n",
        "    #There are other smoothing methods, generally suitable for other, more complex applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "j5_hEHDSwGn7",
        "outputId": "f4ee94c9-f040-47a8-a601-a27fb8fbb8f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'## Detail: training\\n\",\\n    \"\\n\",\\n    \"The idea is to extract the number of occurrences $T(w,c)$ for each word $w$ and each class $c$, which will make it possible to calculate the matrix of conditional probabilities $\\\\pmb{P}$ such that: $$\\\\pmb{P}_{w,c} = P(w|c)$$\\n\",\\n    \"\\n\",\\n    \"Note that the number of occurrences $T(w,c)$ can be easily obtained from the BoW representations of all documents !\\n\",\\n    \"\\n\",\\n    \"### Procedure:\\n\",\\n    \"\\n\",\\n    \"- Extract the vocabulary $V$ and counts $T(w,c)$ for each of the words $w$ and classes $c$, from a set of documents.\\n\",\\n    \"- Calculate the a priori probabilities of the classes $P(c) = \\\\frac{\\\\sum_{w \\\\in V} T(w,c)}{\\\\sum_{c \\\\in C} \\\\sum_{w \\\\in V} T(w,c)}$\\n\",\\n    \"- Calculate the conditional **smoothed** probabilities $P(w|c) = \\\\frac{T(w,c) + 1}{\\\\sum_{w\\' \\\\in V} T(w\\',c) + 1}$.\\n\",\\n    \"\\n\",\\n    \"## Detail: test\\n\",\\n    \"\\n\",\\n    \"We now know the conditional probabilities given by the $\\\\pmb{P}$ matrix. \\n\",\\n    \"Now we must obtain $P(s|c)$ for the current document. This quantity is obtained using a simple calculation involving the BoW representation of the document and $\\\\pmb{P}$.\\n\",\\n    \"\\n\",\\n    \"### Procedure:\\n\",\\n    \"\\n\",\\n    \"- For each of the classes $c$,\\n\",\\n    \"    - $Score(c) = \\\\log P(c)$\\n\",\\n    \"    - For each word $w$ in the document to be tested:\\n\",\\n    \"        - $Score(c) += \\\\log P(w|c)$\\n\",\\n    \"- Return $argmax_{c \\\\in C} Score(c)$ \"'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''## Detail: training\\n\",\n",
        "    \"\\n\",\n",
        "    \"The idea is to extract the number of occurrences $T(w,c)$ for each word $w$ and each class $c$, which will make it possible to calculate the matrix of conditional probabilities $\\\\pmb{P}$ such that: $$\\\\pmb{P}_{w,c} = P(w|c)$$\\n\",\n",
        "    \"\\n\",\n",
        "    \"Note that the number of occurrences $T(w,c)$ can be easily obtained from the BoW representations of all documents !\\n\",\n",
        "    \"\\n\",\n",
        "    \"### Procedure:\\n\",\n",
        "    \"\\n\",\n",
        "    \"- Extract the vocabulary $V$ and counts $T(w,c)$ for each of the words $w$ and classes $c$, from a set of documents.\\n\",\n",
        "    \"- Calculate the a priori probabilities of the classes $P(c) = \\\\frac{\\\\sum_{w \\\\in V} T(w,c)}{\\\\sum_{c \\\\in C} \\\\sum_{w \\\\in V} T(w,c)}$\\n\",\n",
        "    \"- Calculate the conditional **smoothed** probabilities $P(w|c) = \\\\frac{T(w,c) + 1}{\\\\sum_{w' \\\\in V} T(w',c) + 1}$.\\n\",\n",
        "    \"\\n\",\n",
        "    \"## Detail: test\\n\",\n",
        "    \"\\n\",\n",
        "    \"We now know the conditional probabilities given by the $\\\\pmb{P}$ matrix. \\n\",\n",
        "    \"Now we must obtain $P(s|c)$ for the current document. This quantity is obtained using a simple calculation involving the BoW representation of the document and $\\\\pmb{P}$.\\n\",\n",
        "    \"\\n\",\n",
        "    \"### Procedure:\\n\",\n",
        "    \"\\n\",\n",
        "    \"- For each of the classes $c$,\\n\",\n",
        "    \"    - $Score(c) = \\\\log P(c)$\\n\",\n",
        "    \"    - For each word $w$ in the document to be tested:\\n\",\n",
        "    \"        - $Score(c) += \\\\log P(w|c)$\\n\",\n",
        "    \"- Return $argmax_{c \\\\in C} Score(c)$ \"'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "ID-os2QxwGrT",
        "outputId": "de8e2548-49c1-4cc1-ec2e-7552265879cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'We will therefore be able to use the reviews at our disposal to **estimate the probabilities \\n  $P(w|c)$ for each word $w$ given the two classes $c$**. These reviews will allow us to learn how \\n  to evaluate the \"compatibility\" between words and classes.\\n\",\\n    \"```python\\n\",\\n    \"def fit(self, X, y)\\n\",\\n    \"``` \\n\",\\n    \"**Training**: will learn a statistical model based on the representations $X$ corresponding \\n    to the labels $y$.\\n\",\\n    \"Here, $X$ contains representations obtained as the output of ```count_words```. You can complete\\n     the function using the procedure detailed above. \\n\",\\n    \"\\n\",\\n    \"Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\\\alpha$,\\n     which we can implement as an argument of the class \"NB\".\\n\",\\n    \"\\n\",\\n    \"```python\\n\",\\n    \"def predict(self, X)\\n\",\\n    \"```\\n\",\\n    \"**Testing**: will return the labels predicted by the model for other representations $X$.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''We will therefore be able to use the reviews at our disposal to **estimate the probabilities \n",
        "  $P(w|c)$ for each word $w$ given the two classes $c$**. These reviews will allow us to learn how \n",
        "  to evaluate the \\\"compatibility\\\" between words and classes.\\n\",\n",
        "    \"```python\\n\",\n",
        "    \"def fit(self, X, y)\\n\",\n",
        "    \"``` \\n\",\n",
        "    \"**Training**: will learn a statistical model based on the representations $X$ corresponding \n",
        "    to the labels $y$.\\n\",\n",
        "    \"Here, $X$ contains representations obtained as the output of ```count_words```. You can complete\n",
        "     the function using the procedure detailed above. \\n\",\n",
        "    \"\\n\",\n",
        "    \"Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\\\alpha$,\n",
        "     which we can implement as an argument of the class \\\"NB\\\".\\n\",\n",
        "    \"\\n\",\n",
        "    \"```python\\n\",\n",
        "    \"def predict(self, X)\\n\",\n",
        "    \"```\\n\",\n",
        "    \"**Testing**: will return the labels predicted by the model for other representations $X$.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzPg0PZL4f4o"
      },
      "outputs": [],
      "source": [
        "class NB(BaseEstimator, ClassifierMixin):\n",
        "        # Les arguments de classe permettent l'h√©ritage de classes de sklearn\\n\",\n",
        "         def __init__(self, alpha=1.0):\n",
        "    # alpha est un param√®tre pour le lissage (smoothing). Dans l'algorithme d'entra√Ænement, et comme valeur par d√©faut, on utilise alpha = 1'''\n",
        "            self.alpha = alpha\n",
        "\n",
        "         def fit(self, X, y):\n",
        "            n_samples, n_features = X.shape\n",
        "    \n",
        "            classes = np.unique(y) # all labels\n",
        "            n_classes = len(classes) # number of labels\n",
        "\n",
        "            prior = np.zeros(n_classes) # probabilit√©s des classes √† priori\\n\",\n",
        "\n",
        "            tct = np.zeros((n_classes, n_features))\n",
        "            for k, c in enumerate(classes):\n",
        "                prior[k] = np.sum(y == c) / float(n_samples) # a priori, ce sont les fr√©quences\\n\",\n",
        "                tct[k, :] = np.sum(X[y == c, :], axis=0) # nombre(words|classe)\\n\",\n",
        "   \n",
        "            alpha = self.alpha  # laplace smoothing / lissage de laplace\\n\",\n",
        "            cond_prob = (tct + alpha) / np.sum(tct + alpha, axis=1)[:, None]\n",
        "\n",
        "            self.prior_ = prior\n",
        "            self.log_cond_prob_ = np.log(cond_prob) # On enregistre les probabilit√©s conditionelles dans un attribut de la classe (avec self)\\n\",\n",
        "            return self\n",
        "\n",
        "         def predict(self, X):\n",
        "            n_classes = len(self.prior_)\n",
        "            scores = np.dot(X, self.log_cond_prob_.T) # On se sert de l'attribut appris dans \\\"fit\\\"\\n\",\n",
        "            scores += np.log(self.prior_)[None, :]\n",
        "            return np.array([0, 1])[np.argmax(scores, axis=1)]\n",
        "  \n",
        "         def score(self, X, y):\n",
        "            return np.mean(self.predict(X) == y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gXshyXR4f-v"
      },
      "outputs": [],
      "source": [
        "## Experimentation\"\n",
        "\n",
        "nb = NB()\n",
        "nb.fit(train_bow, train_labels_splt)\n",
        "val_pred = nb.predict(val_bow)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioLl0CMW-4MM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "YIprT04o_Otl",
        "outputId": "f4e0a000-d982-44f1-84fa-7edb4a936c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       256\n",
            "           1       0.82      0.75      0.78       244\n",
            "\n",
            "    accuracy                           0.80       500\n",
            "   macro avg       0.80      0.79      0.80       500\n",
            "weighted avg       0.80      0.80      0.80       500\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(classification_report(val_labels, val_pred))\n",
        "cm = confusion_matrix(val_labels , val_pred, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1y-u7jA_OwV",
        "outputId": "7ecbb299-2671-444e-9825-fd009c08b4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       256\n",
            "           1       0.82      0.75      0.78       244\n",
            "\n",
            "    accuracy                           0.80       500\n",
            "   macro avg       0.80      0.79      0.80       500\n",
            "weighted avg       0.80      0.80      0.80       500\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf_nb = MultinomialNB()\n",
        "clf_nb.fit(train_bow, train_labels_splt)\n",
        "val_pred = clf_nb.predict(val_bow)\n",
        "print(classification_report(val_labels, val_pred))\n",
        "cm = confusion_matrix(val_labels , val_pred, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV3t3tAG_O0D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}